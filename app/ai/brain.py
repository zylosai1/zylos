"""
Zylos Brain — The Core AI Engine
Responsible for:
- Understanding user message
- Building context
- Using planner to decide next actions
- Calling tools
- Doing LLM reasoning
- Reflection (retry strategy)
- Memory saving + retrieval
- Persona injection
- Returning final polished answer
"""

from typing import List, Dict, Any
from sqlmodel import Session

from app.ai.llm_local import call_local_llm
from app.ai.planner import simple_plan, llm_plan
from app.ai.tools.tool_router import call_tool
from app.ai.prompt_engine import (
    build_system_prompt,
    build_prompt_with_context
)
from app.ai.reflection import reflect_and_retry
from app.ai.summarizer import summarize_text
from app.ai.memory_engine import (
    get_relevant_memory,
    add_memory_item
)
from app.database import crud

# ---------------------------------------------------------
# CONFIGS
# ---------------------------------------------------------
MAX_HISTORY = 10
USE_LLM_PLANNER_IF_NEEDED = True


# ---------------------------------------------------------
#  Helper: Convert DB messages to simple dicts
# ---------------------------------------------------------
def format_history(messages):
    hist = []
    for m in messages:
        hist.append({
            "role": m.role if m.role != "zylos" else "assistant",
            "text": m.text
        })
    return hist


# ---------------------------------------------------------
# Execute steps generated by planner
# ---------------------------------------------------------
def execute_step(step: Dict[str, Any], user, conversation, session: Session):
    action = step.get("action")

    # --------------------------------------------
    # TOOL CALL
    # --------------------------------------------
    if action == "call_tool":
        tool = step.get("tool")
        args = step.get("args", {})
        try:
            result = call_tool(tool, **args)
        except Exception as e:
            return f"[Tool error: {tool}] {str(e)}"
        return result

    # --------------------------------------------
    # LLM Reasoning step
    # --------------------------------------------
    if action == "llm_reason":
        text = step.get("args", {}).get("text")
        context_prompt = (
            build_system_prompt(user) +
            f"\nUSER REQUEST: {text}\n\nProvide the best answer you can.\n"
        )
        res = call_local_llm(context_prompt)
        return res

    # --------------------------------------------
    # Unknown step
    # --------------------------------------------
    return f"[Unknown planner step: {action}]"


# ---------------------------------------------------------
#  MAIN BRAIN FUNCTION
# ---------------------------------------------------------
def process_user_message(user, conversation, text: str):
    """
    Entry point used by routes_chat.
    (1) Load history
    (2) Get memory
    (3) Run planner
    (4) Execute steps (tool or llm)
    (5) Reflection fix (retry if needed)
    (6) Save memory from final output
    """
    # DB session created by caller (routes_chat)
    from sqlmodel import Session
    from app.database.base import engine

    with Session(engine) as session:

        # ----------------------------
        # LOAD RECENT HISTORY
        # ----------------------------
        history_db = crud.get_last_messages(session, conversation.id, limit=MAX_HISTORY)
        short_history = format_history(history_db)

        # ----------------------------
        # GET RELEVANT MEMORY
        # ----------------------------
        memory_snippets = get_relevant_memory(user.id, text)

        # ----------------------------
        # BUILD SYSTEM + FULL PROMPT
        # ----------------------------
        system_prompt = build_system_prompt(user)
        full_prompt = build_prompt_with_context(
            system_prompt,
            short_history,
            text,
            memory_snippets
        )

        # ----------------------------
        # PLANNER DECISION
        # ----------------------------
        plan = simple_plan(text)

        if plan["plan_type"] == "llm" and USE_LLM_PLANNER_IF_NEEDED:
            plan = llm_plan(text, user=user)

        # ----------------------------
        # EXECUTE PLANNER STEPS
        # ----------------------------
        results = []
        if plan.get("steps"):
            for step in plan["steps"]:
                try:
                    out = execute_step(step, user, conversation, session)
                    results.append(out)
                except Exception as e:
                    results.append(f"[step execution error] {str(e)}")

        # If using llm_plan fallback
        if plan.get("plan_type") == "llm" and plan.get("llm_plan_text"):
            results.append(plan["llm_plan_text"])

        combined_reply = "\n".join([r for r in results if r])

        # ----------------------------
        # REFLECTION IMPROVEMENT
        # ----------------------------
        improved, new_reply = reflect_and_retry(text, combined_reply, user)
        final_reply = new_reply if improved else combined_reply

        # ----------------------------
        # SAVE MEMORY (long-term)
        # ----------------------------
        try:
            add_memory_item(user.id, text, final_reply)
        except Exception:
            pass

        # ----------------------------
        # Edge-case: empty answer fallback
        # ----------------------------
        if not final_reply or len(final_reply.strip()) < 2:
            final_reply = "Sorry, मुझे पूरा समझ नहीं आया — थोड़ा और detail में बताओ?"

        return final_reply